# Configuration README

This document provides an overview of the configuration parameters used in the codebase. It covers parameters actively implemented in various modules, as well as those that are defined but partially implemented or not currently in use.

---

## Actively Implemented Configuration Parameters

### Learning Rate (`learning_rate`)
- **Usage**: Utilized in `optimizer.c` for parameter updates in both Bayesian linear and stochastic activation layers.
- **Effect**: Influences the speed and stability of training.

### KL Weight (`kl_weight`)
- **Usage**: Employed in `bayesian_linear.c` and `stochastic_activation.c` for KL divergence calculations.
- **Effect**: Balances data fit and prior regularization, affecting the Bayesian regularization strength.

### Dropout Probability (`dropout_prob`)
- **Usage**: Applied in `network.c` when creating dropout layers.
- **Effect**: Controls the probability of dropping neurons during training.

### Prior Variance (`prior_variance`)
- **Usage**: Used in `bayesian_linear.c` and `stochastic_activation.c` for KL divergence calculations.
- **Effect**: Determines the spread of the prior distribution.

### Prior Type (`prior_type`)
- **Usage**: Configured in `network.c` to select the type of prior (Gaussian, Laplace, or Mixture).
- **Effect**: Defines the shape of the prior distribution.

### Posterior Method (`posterior_method`)
- **Usage**: Defined in `network.c` to choose the posterior approximation method (Mean-field, Structured, or Flipout).
- **Effect**: Influences how the network samples from the posterior.

### Number of Layers (`num_layers`)
- **Usage**: Set in `network.c` to determine the network's depth.
- **Effect**: Impacts the overall architecture by defining the number of layers.

### Layer Types (`layer_types`)
- **Usage**: Configured in `network.c` to specify the types of layers to create.
- **Effect**: Shapes the network architecture.

### Neurons per Layer (`neurons_per_layer`)
- **Usage**: Defined in `network.c` for determining the size of each layer.
- **Effect**: Affects the width of the network.

### Number of Epochs (`num_epochs`)
- **Usage**: Utilized in `stochastic_activation.c` for KL annealing.
- **Effect**: Sets the training duration.

### KL Annealing (`kl_annealing`)
- **Usage**: Applied in `stochastic_activation.c` to scale the KL divergence over training.
- **Effect**: Adjusts how Bayesian regularization strength changes during training.

### Gradient Clipping (`grad_clip`)
- **Usage**: Used in `stochastic_activation.c` to limit the magnitude of gradients.
- **Effect**: Enhances training stability by preventing exploding gradients.

---

## Partially Implemented or Unused Configuration Variables

- **Mini-batch Size (`mini_batch_size`)**: Defined but not actively used in training loops.
- **Learning Rate Decay (`lr_decay`)**: Defined but not implemented in the optimizer.
- **Optimizer Type (`optimizer`)**: Only SGD is implemented; alternative optimizers are not currently available.
- **Noise Injection (`noise_injection`)**: Defined but not implemented in training.
- **Inference Method (`inference_method`)**: Defined, with only BBB (Bayes-by-backprop) fully implemented.
- **Weight Initialization Method (`weight_init_method`)**: Defined but not fully implemented in layer creation.
- **Covariance Structure (`covariance_structure`)**: Defined but not implemented in the network.
- **MC Samples for Training (`mc_samples_train`)**: Defined but not fully utilized.
- **Local Reparameterization (`local_reparam`)**: Defined but not implemented.
- **MC Samples for Inference (`mc_samples_inference`)**: Defined but not fully implemented.
- **MCMC-related Parameters**:
  - `mcmc_step_size`
  - `mcmc_burn_in`
  - `mcmc_noise`
  
  (All defined but MCMC inference is not implemented.)

- **EP-related Parameters**:
  - `ep_damping`
  - `ep_iterations`
  - `ep_tolerance`
  
  (All defined but Expectation Propagation is not implemented.)

- **Sampling Temperature (`sampling_temperature`)**: Defined but not implemented.
- **Regularization Weight (`regularization_weight`)**: Defined but not implemented.
- **BBB-specific Extras**:
  - `bbb_learn_variance`
  - `bbb_noise_scaling`
  
  (Defined but not fully implemented.)
- **Ensemble Size (`ensemble_size`)**: Defined but ensemble methods are not implemented.

---

## Notes

- The parameters in the **Actively Implemented** section directly influence training dynamics and network architecture.
- The **Partially Implemented or Unused** section lists parameters that are either reserved for future development or require further implementation.
- Always verify that parameter changes align with your overall training and inference strategies.

Feel free to update and expand this README as your configuration implementations evolve.
