Adam Optimizer Fundamentals
The Adam optimizer is widely used in deep learning due to its adaptive learning rate and efficient handling of sparse gradients. Its key elements include:

First and Second Moments:

m (First Moment): An exponentially decaying average of past gradients.

v (Second Moment): An exponentially decaying average of the squared gradients.

Bias Correction: Because the moment estimates are initialized at zero, bias-corrected estimates (mÌ‚ and vÌ‚) are computed to avoid initial underestimation.

Update Rule: For a parameter 
ğœƒ
Î¸ at time step 
ğ‘¡
t, the update is computed as follows:

ğ‘š
ğ‘¡
=
ğ›½
1
â‹…
ğ‘š
ğ‘¡
âˆ’
1
+
(
1
âˆ’
ğ›½
1
)
â‹…
ğ‘”
ğ‘¡
,
ğ‘£
ğ‘¡
=
ğ›½
2
â‹…
ğ‘£
ğ‘¡
âˆ’
1
+
(
1
âˆ’
ğ›½
2
)
â‹…
ğ‘”
ğ‘¡
2
,
ğ‘š
^
ğ‘¡
=
ğ‘š
ğ‘¡
1
âˆ’
ğ›½
1
ğ‘¡
,
ğ‘£
^
ğ‘¡
=
ğ‘£
ğ‘¡
1
âˆ’
ğ›½
2
ğ‘¡
,
ğœƒ
ğ‘¡
+
1
=
ğœƒ
ğ‘¡
âˆ’
ğ›¼
â‹…
ğ‘š
^
ğ‘¡
ğ‘£
^
ğ‘¡
+
ğœ–
,
m 
t
â€‹
 
v 
t
â€‹
 
m
^
  
t
â€‹
 
Î¸ 
t+1
â€‹
 
â€‹
  
=Î² 
1
â€‹
 â‹…m 
tâˆ’1
â€‹
 +(1âˆ’Î² 
1
â€‹
 )â‹…g 
t
â€‹
 ,
=Î² 
2
â€‹
 â‹…v 
tâˆ’1
â€‹
 +(1âˆ’Î² 
2
â€‹
 )â‹…g 
t
2
â€‹
 ,
= 
1âˆ’Î² 
1
t
â€‹
 
m 
t
â€‹
 
â€‹
 , 
v
^
  
t
â€‹
 = 
1âˆ’Î² 
2
t
â€‹
 
v 
t
â€‹
 
â€‹
 ,
=Î¸ 
t
â€‹
 âˆ’Î±â‹… 
v
^
  
t
â€‹
 
â€‹
 +Ïµ
m
^
  
t
â€‹
 
â€‹
 ,
â€‹
 
where:

ğ›¼
Î± is the learning rate.

ğ›½
1
Î² 
1
â€‹
  and 
ğ›½
2
Î² 
2
â€‹
  are decay rates for the moments.

ğœ–
Ïµ is a small constant to prevent division by zero.

4. Integrating Adam with a Bayesian Neural Network
When applying Adam within the framework of a Bayesian neural network, consider the following aspects:

Parameterization of the Posterior:
The parameters being optimized are those of your approximate posterior distribution (e.g., the means and log-variances of the weight distributions). Adam will update these variational parameters.

Gradient Computation on the ELBO:
Instead of a standard loss function (like cross-entropy), youâ€™re computing gradients of the ELBO with respect to the variational parameters. This includes:

Gradients from the data likelihood component.

Gradients from the KL divergence term.

Stochastic Gradient Estimates:
Due to the use of Monte Carlo sampling (enabled by the reparameterization trick), the gradient estimates will be stochastic. Adamâ€™s adaptive nature helps in smoothing these noisy updates.

Hyperparameter Tuning:
Typical Adam settings (e.g., 
ğ›¼
=
0.001
Î±=0.001, 
ğ›½
1
=
0.9
Î² 
1
â€‹
 =0.9, 
ğ›½
2
=
0.999
Î² 
2
â€‹
 =0.999, 
ğœ–
=
10
âˆ’
8
Ïµ=10 
âˆ’8
 ) often serve as a good starting point, but you might need to adjust them considering the added complexity of the Bayesian formulation.

